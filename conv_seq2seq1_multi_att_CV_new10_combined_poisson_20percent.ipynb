{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os.path\n",
    "file_name = 'resampled_2x5_cross_validation'\n",
    "if os.path.isfile(file_name+'.pickle'):\n",
    "    with open(file_name+'.pickle', 'rb') as handle:\n",
    "        train_folds,test_folds = pickle.load(handle)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from attentions import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=None, stride=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv2(self.relu(self.conv1(x))))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 64), kernel_size=3, stride=None):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "            [Block(chs[i], chs[i + 1], kernel_size=kernel_size, stride=stride) for i in range(len(chs) - 1)])\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x.flatten(1, -1))\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class RnnType:\n",
    "    GRU = 1\n",
    "    LSTM = 2\n",
    "\n",
    "\n",
    "class AttentionModel:\n",
    "    NONE = 0\n",
    "    DOT = 1\n",
    "    GENERAL = 2\n",
    "    CONCAT = 3\n",
    "\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, data_dict):\n",
    "        for k, v in data_dict.items():\n",
    "            exec(\"self.%s=%s\" % (k, v))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.init_hidden(input.shape[0])\n",
    "        output, hidden = self.net(input,\n",
    "                                  hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.method == 'GRU':\n",
    "            return torch.zeros(self.n_layers * self.n_directions,\n",
    "                               batch_size,\n",
    "                               self.hidden_size).to(self.device)\n",
    "        elif self.method == 'LSTM':\n",
    "            return (torch.zeros(self.n_layers * self.n_directions,\n",
    "                                batch_size,\n",
    "                                self.hidden_size).to(self.device),\n",
    "                    torch.zeros(self.n_layers * self.n_directions,\n",
    "                                batch_size,\n",
    "                                self.hidden_size).to(self.device))\n",
    "        else:\n",
    "            raise Exception('Unknown rnn_type. Valid options: \"gru\", \"lstm\"')\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, device, params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "\n",
    "        # Calculate number of directions\n",
    "        self.num_directions = 2 if self.params.bidirectional == True else 1\n",
    "        self.encoder = Encoder(chs=self.params.chs_encoder,\n",
    "                               kernel_size=self.params.kernel_size,\n",
    "                               stride=self.params.stride)\n",
    "        # Attention layer\n",
    "        self.attention = MultiHeadAttention(d_model=self.params.rnn_hidden_dim * self.num_directions, num_heads=4)\n",
    "\n",
    "        # RNN layer\n",
    "        rnn = None\n",
    "        if self.params.rnn_type == RnnType.GRU:\n",
    "            rnn = nn.GRU\n",
    "            self.decoder_hidden = nn.Sequential(\n",
    "                torch.nn.LazyLinear(self.params.layer_width),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.params.layer_width,\n",
    "                                self.num_directions * self.params.num_layers * self.params.rnn_hidden_dim),\n",
    "                torch.nn.Sigmoid()\n",
    "            )\n",
    "        elif self.params.rnn_type == RnnType.LSTM:\n",
    "            rnn = nn.LSTM\n",
    "            self.decoder_hidden1 = nn.Sequential(\n",
    "                torch.nn.LazyLinear(self.params.layer_width),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.params.layer_width,\n",
    "                                self.num_directions * self.params.num_layers * self.params.rnn_hidden_dim),\n",
    "                torch.nn.Sigmoid()\n",
    "            )\n",
    "            self.decoder_hidden2 = nn.Sequential(\n",
    "                torch.nn.LazyLinear(self.params.layer_width),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.params.layer_width,\n",
    "                                self.num_directions * self.params.num_layers * self.params.rnn_hidden_dim),\n",
    "                torch.nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"[Error] Unknown RnnType. Currently supported: RnnType.GRU=1, RnnType.LSTM=2\")\n",
    "        self.rnn = rnn(input_size=self.params.rnn_hidden_dim,\n",
    "                       hidden_size=self.params.rnn_hidden_dim,\n",
    "                       num_layers=self.params.num_layers,\n",
    "                       bidirectional=self.params.bidirectional,\n",
    "                       dropout=self.params.dropout,\n",
    "                       batch_first=True)\n",
    "        # self.rnn2 = rnn(input_size=self.params.rnn_hidden_dim*self.num_directions,\n",
    "        #                hidden_size=self.params.rnn_hidden_dim,\n",
    "        #                num_layers=self.params.num_layers,\n",
    "        #                bidirectional=self.params.bidirectional,\n",
    "        #                dropout=self.params.dropout,\n",
    "        #                batch_first=True)\n",
    "\n",
    "        self.decoder_input = nn.Sequential(\n",
    "            torch.nn.LazyLinear(self.params.layer_width),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.params.layer_width, self.params.output_sequence_length * self.params.rnn_hidden_dim),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            torch.nn.Linear(self.params.rnn_hidden_dim * self.num_directions, self.params.layer_width),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.params.layer_width, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.encoder_output_linear = nn.Sequential(\n",
    "            nn.LazyLinear(self.params.output_sequence_length*self.params.rnn_hidden_dim*2)\n",
    "        )\n",
    "        self.to(device)\n",
    "\n",
    "    def init_hidden(self, encoder_outputs):\n",
    "        if self.params.rnn_type == RnnType.GRU:\n",
    "            hidden = self.decoder_hidden(encoder_outputs).view(self.batch_size,\n",
    "                                                               self.params.num_layers * self.num_directions,\n",
    "                                                               self.params.rnn_hidden_dim).transpose(0, 1)\n",
    "            return hidden.contiguous().to(self.device)\n",
    "        elif self.params.rnn_type == RnnType.LSTM:\n",
    "            hidden1 = self.decoder_hidden1(encoder_outputs).view(self.batch_size,\n",
    "                                                                 self.params.num_layers * self.num_directions,\n",
    "                                                                 self.params.rnn_hidden_dim).transpose(0, 1)\n",
    "            hidden2 = self.decoder_hidden2(encoder_outputs).view(self.batch_size,\n",
    "                                                                 self.params.num_layers * self.num_directions,\n",
    "                                                                 self.params.rnn_hidden_dim).transpose(0, 1)\n",
    "            return (hidden1.contiguous().to(self.device),\n",
    "                    hidden2.contiguous().to(self.device)\n",
    "                    )\n",
    "        else:\n",
    "            raise Exception('Unknown rnn_type. Valid options: \"gru\", \"lstm\"')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.batch_size, seq_len = inputs.shape  # to encoder\n",
    "        encoder_outputs = self.encoder(inputs.unsqueeze(1))\n",
    "        encoder_outputs = torch.cat(encoder_outputs, 1)\n",
    "        # encoder_outputs (batch_size, N)\n",
    "        # decoder_inputs: (batch_size, sequence length, hidden)\n",
    "        decoder_inputs = self.decoder_input(encoder_outputs).view(self.batch_size,\n",
    "                                                                  self.params.output_sequence_length,\n",
    "                                                                  self.params.rnn_hidden_dim)\n",
    "        \n",
    "        self.hidden = self.init_hidden(encoder_outputs)\n",
    "        \n",
    "        # Push through RNN layer\n",
    "        rnn_output, _ = self.rnn(decoder_inputs, self.hidden)\n",
    "\n",
    "        # (batch_size, output_seq_len, num_directions*hidden)\n",
    "        decoder_inputs_ = torch.cat([decoder_inputs, decoder_inputs], 2)\n",
    "        \n",
    "        encoder_output_ = self.encoder_output_linear(encoder_outputs).view(self.batch_size,\n",
    "                                                                  self.params.output_sequence_length,\n",
    "                                                                  self.params.rnn_hidden_dim*2)\n",
    "        \n",
    "\n",
    "        context, att = self.attention(rnn_output, encoder_output_, encoder_output_)\n",
    "        #context, att = self.attention(rnn_output, decoder_inputs_, decoder_inputs_)\n",
    "\n",
    "        #context = self.rnn2(context, self.hidden)[0]\n",
    "        #residual connection\n",
    "        #context = context + rnn_output\n",
    "        # print(rnn_output.shape)\n",
    "        spectrum_out = self.linear(context).squeeze(2) + rnn_output.mean(dim=2)\n",
    "        return spectrum_out\n",
    "\n",
    "\n",
    "class XSigmoidLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        # return torch.mean(2 * ey_t / (1 + torch.exp(-ey_t)) - ey_t)\n",
    "        return torch.mean(2 * ey_t * torch.sigmoid(ey_t) - ey_t)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "class CalculateMSE():\n",
    "    def __init__(self, net,n_epochs,batch_size ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        #initialize some constants\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 1e-4\n",
    "        self.n_epochs = n_epochs\n",
    "        self.net.apply(self.weights_init)\n",
    "    def weights_init(self,layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            nn.init.orthogonal_(layer.weight)\n",
    "    def get_mse(self,train_data, train_label, test_data, test_label):\n",
    "        train_set = torch.utils.data.TensorDataset(\n",
    "            torch.Tensor(train_data),\n",
    "            torch.Tensor(train_label))\n",
    "        val_set = torch.utils.data.TensorDataset(\n",
    "            torch.Tensor(test_data),\n",
    "            torch.Tensor(test_label))\n",
    "        loader_args = dict(batch_size=self.batch_size)\n",
    "        train_loader = DataLoader(train_set, shuffle=True, drop_last=True, **loader_args)\n",
    "        val_loader = DataLoader(val_set, shuffle=True, drop_last=True, **loader_args)\n",
    "        tloss = []\n",
    "        vloss = []\n",
    "        criterion = XSigmoidLoss()\n",
    "        default_criterion = nn.MSELoss()\n",
    "        #add weight decay\n",
    "        optimizer = optim.Adam(self.net.parameters(), lr=self.learning_rate, weight_decay=0) # weight_decay=5e-4\n",
    "        # optimizer = optim.Adam(self.net.parameters(), lr=self.learning_rate) # weight_decay=0\n",
    "        with tqdm(range(0, self.n_epochs)) as pbar:\n",
    "            for epoch in pbar:\n",
    "                epoch_train_loss=[]\n",
    "                for i, data in enumerate(train_loader, 0):\n",
    "                    inputs, label = data\n",
    "                    y_pred = self.net(inputs.to(self.net.device))\n",
    "                    loss = criterion(y_pred, label.to(self.net.device))\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_train_loss.append(default_criterion(y_pred, label.to(self.net.device)).item())\n",
    "                tloss.append(np.mean(epoch_train_loss))\n",
    "                epoch_loss=[]\n",
    "                for i, data in enumerate(val_loader, 0):\n",
    "                    with torch.no_grad():\n",
    "                        inputs1, label1 = data\n",
    "                        y_pred1 = self.net(inputs1.to(self.net.device))\n",
    "                        loss1 = default_criterion(y_pred1, label1.to(self.net.device))\n",
    "                        epoch_loss.append(loss1.item())\n",
    "                vloss.append(np.mean(epoch_loss))\n",
    "                pbar.set_postfix({'EPOCH':epoch,\n",
    "                      'tr_loss':tloss[-1],\n",
    "                      'val_loss':vloss[-1]})\n",
    "        return np.min(vloss), self.net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "n_epochs=3000\n",
    "batch_size=32\n",
    "PATH = 'saved_model/conv_seq2seq_new10_combined_poisson_20percent/'\n",
    "Path(PATH).mkdir(parents=True, exist_ok=True)\n",
    "params = {\n",
    "    'chs_encoder':(1,32,64),  #tuple of channels/feature dimensions\n",
    "    'kernel_size':4,\n",
    "    'stride':1,\n",
    "    'rnn_type':RnnType.GRU,\n",
    "    'rnn_hidden_dim': 32,\n",
    "    'layer_width':2000,\n",
    "    'num_layers':1,\n",
    "    'dropout':0,\n",
    "    'attention_model':AttentionModel.GENERAL,\n",
    "    'sequence_length':64,\n",
    "    'output_sequence_length':250,\n",
    "    'batch_size':32,\n",
    "    'bidirectional':True}\n",
    "params  =Parameters(params)\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2013326918368819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/junbo.s/.conda/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "100%|██████████| 3000/3000 [13:33<00:00,  3.69it/s, EPOCH=2999, tr_loss=5.2e-6, val_loss=0.0184]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.012557580322027206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/junbo.s/.conda/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19977448149890295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [13:40<00:00,  3.66it/s, EPOCH=2999, tr_loss=1.23e-6, val_loss=0.0218] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.01793466005474329\n",
      "0.20084773771969924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [13:51<00:00,  3.61it/s, EPOCH=2999, tr_loss=7.26e-7, val_loss=0.0282] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.018043005652725697\n",
      "0.20054367390161468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [14:02<00:00,  3.56it/s, EPOCH=2999, tr_loss=3.36e-5, val_loss=0.0198] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.01567353876307607\n",
      "0.20138248748071347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [13:53<00:00,  3.60it/s, EPOCH=2999, tr_loss=1.6e-6, val_loss=0.0229]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.013748646154999733\n",
      "0.20008319727643362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [13:48<00:00,  3.62it/s, EPOCH=2999, tr_loss=1.71e-6, val_loss=0.0253] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.016765367425978183\n",
      "0.1997732757550439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [13:52<00:00,  3.60it/s, EPOCH=2999, tr_loss=5.17e-6, val_loss=0.026]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.01931618880480528\n",
      "0.20122424027711283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [13:47<00:00,  3.62it/s, EPOCH=2999, tr_loss=1.59e-5, val_loss=0.0171] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.011764382570981979\n",
      "0.20072605348301625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [13:43<00:00,  3.64it/s, EPOCH=2999, tr_loss=1.4e-5, val_loss=0.0171]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.013531330227851867\n",
      "0.1999563600361216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [13:25<00:00,  3.73it/s, EPOCH=2999, tr_loss=8.1e-6, val_loss=0.0289]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.01960822716355324\n",
      "0.015894292714074252 0.0027104879603865287\n",
      "0.20056441992655402\n"
     ]
    }
   ],
   "source": [
    "def add_noise(inputs,inputs2, a = 0.02, std = 0.02, sequence_length=64, noise_seed=None):\n",
    "    if noise_seed is not None:\n",
    "        np.random.seed(noise_seed)\n",
    "    noise = np.random.normal(0,std, size = (inputs.shape[0], sequence_length)).astype(np.float32)\n",
    "    noise2 = np.random.normal(0,std, size = (inputs2.shape[0], sequence_length)).astype(np.float32)\n",
    "    #noise = poisson.rvs(mu, size=(inputs.shape[0], sequence_length)).astype(np.float32)\n",
    "    \n",
    "    nsd = a*np.random.poisson(inputs/a).astype(np.float32)\n",
    "    nsd2 = a*np.random.poisson(inputs2/a).astype(np.float32)\n",
    "    \n",
    "    # Calculate the absolute error between nsd and original inputs\n",
    "    absolute_error = np.abs(nsd - inputs)\n",
    "    absolute_error2 = np.abs(nsd2 - inputs2)\n",
    "    # Sum up the absolute errors\n",
    "    total_error = np.sum(absolute_error)\n",
    "    total_error2 = np.sum(absolute_error2)\n",
    "    # Sum of the original data\n",
    "    total_original = np.sum(inputs)\n",
    "    total_original2 = np.sum(inputs2)\n",
    "\n",
    "    # Calculate the noise ratio\n",
    "    noise_ratio = (total_error+total_error2) / (total_original+total_original2)\n",
    "    \n",
    "    return nsd, nsd2, noise_ratio\n",
    "noise_ratios = []\n",
    "for i,(train,test) in enumerate(zip(train_folds,test_folds)):\n",
    "    train_data, train_label= train[0],train[1]\n",
    "    test_data, test_label= test[0],test[1]\n",
    "    # Adding noise to the train and test data\n",
    "    train_data,test_data,noise_ratio = add_noise(train_data,test_data, a=0.026,std = 0.05, sequence_length=64, noise_seed=i)\n",
    "    print(noise_ratio)\n",
    "    noise_ratios.append(noise_ratio)\n",
    "    mdl =  Decoder(device,params)\n",
    "    mse_calculator = CalculateMSE(mdl,n_epochs,batch_size)\n",
    "    loss,model = mse_calculator.get_mse(train_data, train_label, test_data, test_label)\n",
    "    print(i,loss)\n",
    "    losses.append(loss)\n",
    "    torch.save(model.state_dict(), PATH+'model'+str(i))\n",
    "print(np.mean(losses),np.std(losses))\n",
    "print(np.mean(noise_ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "pearson\n",
      "0.8132896535353398 0.36865967082821144\n",
      "0.009121825612633208 0.07201307640743256\n",
      "kendall\n",
      "0.6485829108848404 0.27968066548882037\n",
      "0.012405151266421804 0.08818218116669196\n",
      "spearman\n",
      "0.7692935051332188 0.3364100037890996\n",
      "0.012666618997786674 0.08767564652770236\n",
      "DTW\n",
      "13.054261039922274 19.37646333707158\n",
      "Absolute Error\n",
      "5\n",
      "0.000869096274737588\n",
      "50\n",
      "0.025436672446575243\n",
      "90\n",
      "0.21368723113695756\n",
      "95\n",
      "0.3651936817657969\n",
      "99\n",
      "0.6285821654908152\n",
      "Distance Correlation\n",
      "0.8850314657236138 0.18191248375985603\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import dcor\n",
    "#pip install dtw-python\n",
    "from dtw import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "n_epochs=3000\n",
    "batch_size=32\n",
    "mdl =  Decoder(device,params)\n",
    "\n",
    "correlation_losses = []\n",
    "def calculate_correlation(model, test_data, test_label):\n",
    "    test_data_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "    construction = model(test_data_tensor).detach().cpu().numpy()\n",
    "   \n",
    "    # Pearson\n",
    "    pearson_coefs = []\n",
    "    pearson_ps = []\n",
    "    \n",
    "    # Kendall\n",
    "    kendall_coefs = []\n",
    "    kendall_ps = []\n",
    "    \n",
    "    # Spearman\n",
    "    spearman_coefs = []\n",
    "    spearman_ps = []\n",
    "    \n",
    "    # Distance Correlation\n",
    "    distance_corr = []\n",
    "    \n",
    "    #DTW distance\n",
    "    alignment = []\n",
    "    \n",
    "    #absolute_error\n",
    "    abs_err = []\n",
    "    \n",
    "    for i in range(test_label.shape[0]):\n",
    "        x1 = construction[i,:]\n",
    "        x2 = test_label[i,:]\n",
    "        \n",
    "        res = stats.pearsonr(x1, x2)\n",
    "        pearson_coefs.append(res[0])\n",
    "        pearson_ps.append(res[1])\n",
    "        \n",
    "        res = stats.kendalltau(x1, x2)\n",
    "        kendall_coefs.append(res[0])\n",
    "        kendall_ps.append(res[1])\n",
    "        \n",
    "        res = stats.spearmanr(x1, x2)\n",
    "        spearman_coefs.append(res[0])\n",
    "        spearman_ps.append(res[1])\n",
    "        \n",
    "        distance_corr.append(dcor.distance_correlation(x1,x2))\n",
    "        \n",
    "        alignment.append(dtw(x1, x2, distance_only=True).distance)\n",
    "        abs_err.append(abs(x1-x2))\n",
    "        \n",
    "    correlation_results = {\n",
    "        'pearson': (pearson_coefs, pearson_ps),\n",
    "        'kendall': (kendall_coefs, kendall_ps),\n",
    "        'spearman': (spearman_coefs, spearman_ps),\n",
    "        'DTW': alignment,\n",
    "        'Absolute Error': abs_err,\n",
    "        'Distance Correlation': distance_corr\n",
    "    }\n",
    "\n",
    "    return correlation_results\n",
    "\n",
    "for i,(train,test) in enumerate(zip(train_folds,test_folds)):\n",
    "    print(i)\n",
    "    train_data, train_label= train[0],train[1]\n",
    "    test_data, test_label= test[0],test[1]\n",
    "    train_data,test_data,noise_ratio = add_noise(train_data,test_data, a=0.026,std = 0.05, sequence_length=64, noise_seed=i)\n",
    "    mdl_name = PATH + 'model' + str(i)\n",
    "    mdl.load_state_dict(torch.load(mdl_name))\n",
    "    mdl.eval()\n",
    "    \n",
    "    correlation_loss = calculate_correlation(mdl, test_data, test_label)\n",
    "    correlation_losses.append(correlation_loss)\n",
    "for key in correlation_losses[0].keys():\n",
    "    print(key)\n",
    "    if key=='Absolute Error':\n",
    "        errors = []\n",
    "        for d in correlation_losses:\n",
    "            errors+=np.concatenate(d[key]).ravel().tolist()\n",
    "        #percentile\n",
    "        percentiles = [5, 50, 90, 95, 99]\n",
    "        for p in percentiles:\n",
    "            print(p)\n",
    "            print(np.percentile(errors, p))\n",
    "    else:\n",
    "        stat, p = [], []\n",
    "        for d in correlation_losses:\n",
    "            if key=='DTW' or key=='Distance Correlation':\n",
    "                stat+=d[key]\n",
    "            else:\n",
    "                stat+=d[key][0]\n",
    "                p+=d[key][1]\n",
    "        print(np.mean(stat),np.std(stat))\n",
    "        if len(p)>0:\n",
    "            print(np.mean(p),np.std(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015894292714074252 0.0027104879603865287\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(losses),np.std(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
